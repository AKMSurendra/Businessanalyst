{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1blwCaQv6TTn8qwZphQmn5xyHUxcZ207i",
      "authorship_tag": "ABX9TyNq2qDBjT4S9c94tf1LAHfg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AKMSurendra/Businessanalyst/blob/main/Pysparknote.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PySpark**\n",
        "\n",
        "create df, diff diff transformation, join, multiple function, execute sql querys, streaming, ml library"
      ],
      "metadata": {
        "id": "dwqOE_aTxIHW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmeXGlLlvI6G",
        "outputId": "2264f54f-4935-4181-9b49-9c98b11b2c62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=f5e794186e6a07614fd27e762b4b1a49b2980b0502e68789cfbdce608d59649e\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()   #create spark session"
      ],
      "metadata": {
        "id": "mm1Z0m0-viRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data = [(1, \"anoj\"), (2, \"mishra\")]\n",
        "#schema = [\"id\", \"name\"]\n",
        "#df = spark.createDataFrame(data, schema)\n",
        "#df.show()\n",
        "\n",
        "\n",
        "#data1 = [{\"id\": 2, \"name\":\"anoj\"},{\"id\": 3, \"name\":\"mishra\"}]\n",
        "#df = spark.createDataFrame(data1)\n",
        "#df.show()\n",
        "#df.printSchema()\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "data2 = [{\"id\": 2, \"name\":\"anoj\"},{\"id\": 3, \"name\":\"mishra\"}]\n",
        "schema2 = StructType([StructField(name=\"id\", dataType=IntegerType()), StructField(name=\"name\", dataType=StringType())])\n",
        "df = spark.createDataFrame(data2, schema2)\n",
        "df.show()\n",
        "df.printSchema()\n",
        "\n",
        "#help(spark.createDataFrame)\n",
        "#help(StructType)\n",
        "#help(spark.read)\n",
        "\n",
        "#type(\"abc\")\n",
        "#type(df)\n",
        "#type(spark)\n",
        "#df.dtypes\n",
        "df.columns\n",
        "#df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbmiLPHQwqH9",
        "outputId": "d5ef8337-5d2c-42f5-85a0-7ed64455c972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  2|  anoj|\n",
            "|  3|mishra|\n",
            "+---+------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['id', 'name']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read CSV file\n",
        "from pyspark.sql.types import *\n",
        "#df1 = spark.read.format('csv').option(key='header', value=True).load(\"/content/sample_data/california_housing_test.csv\")\n",
        "#df1.printSchema()\n",
        "\n",
        "#df2 = spark.read.csv(path=\"/content/drive/MyDrive/ML data/Pokemon.csv\", header=True)\n",
        "#display(df2)\n",
        "#df2.printSchema()\n",
        "\n",
        "df1 = spark.read.option('header', True).csv(\"/content/drive/MyDrive/ML data/Pokemon.csv\")\n",
        "#display(df1)\n",
        "#df1.printSchema()\n",
        "print(df1)\n",
        "\n",
        "#MULTIPLE CSV FILE\n",
        "\n",
        "#df2 = spark.read.csv(path=[\"/content/drive/MyDrive/ML data/Pokemon.csv\", \"/content/drive/MyDrive/ML data/item_popularity.csv\"], header=True)\n",
        "#display(df2)\n",
        "#df2.printSchema()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrUUk1LVCSGM",
        "outputId": "4a3967a5-e9e1-446c-f459-85d94ae7c0ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame[#: string, Name: string, Type 1: string, Type 2: string, Total: string, HP: string, Attack: string, Defense: string, Sp. Atk: string, Sp. Def: string, Speed: string, Generation: string, Legendary: string]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8uYLIlTYQ9ZT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}